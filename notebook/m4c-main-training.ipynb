{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7275033,"sourceType":"datasetVersion","datasetId":4217639},{"sourceId":7327440,"sourceType":"datasetVersion","datasetId":4253079},{"sourceId":7344399,"sourceType":"datasetVersion","datasetId":4264611}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-05T14:18:43.104190Z","iopub.execute_input":"2024-01-05T14:18:43.105362Z","iopub.status.idle":"2024-01-05T14:18:47.992002Z","shell.execute_reply.started":"2024-01-05T14:18:43.105307Z","shell.execute_reply":"2024-01-05T14:18:47.991220Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q transformers\n!pip install -q evaluate\n!pip install -q rouge_score\n!pip install --upgrade nltk\n!pip install -q git+https://github.com/salaniz/pycocoevalcap.git","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:18:47.993644Z","iopub.execute_input":"2024-01-05T14:18:47.994017Z","iopub.status.idle":"2024-01-05T14:20:11.065054Z","shell.execute_reply.started":"2024-01-05T14:18:47.993990Z","shell.execute_reply":"2024-01-05T14:20:11.063955Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.8.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.__version__","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:20:11.066654Z","iopub.execute_input":"2024-01-05T14:20:11.067032Z","iopub.status.idle":"2024-01-05T14:20:12.296367Z","shell.execute_reply.started":"2024-01-05T14:20:11.066996Z","shell.execute_reply":"2024-01-05T14:20:12.295452Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'3.8.1'"},"metadata":{}}]},{"cell_type":"code","source":"import glob\nimport nltk\nimport math\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom random import choice\nfrom tqdm.auto import tqdm\nfrom pycocoevalcap.cider.cider import Cider\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom nltk.tokenize import word_tokenize\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nnltk.download('punkt')\n\nclass ViTextCapsDataset(Dataset):\n  def __init__(self, tokenizer, obj_features_folder=None, ocr_features_folder=None, fasttext_token=None, caption_file=None, convert_obj_bbx=True, data=None, mask_value=64000.0):\n\n    self.convert_obj_bbx = convert_obj_bbx\n    self.tokenizer = tokenizer\n    self.mask_value = mask_value\n    self.dummy_tensor = torch.ones((1, 300))\n\n    # if data is None all others argument cant be None\n    assert (obj_features_folder is not None and ocr_features_folder is not None \\\n    and fasttext_token is not None and caption_file is not None) or data is not None, \"All other arguments must be passed if data is None!\"\n\n    if data is None:\n      self.load_data(obj_features_folder, ocr_features_folder, fasttext_token, caption_file)\n    else:\n      self.data = data\n\n  def load_data(self, obj_features_folder, ocr_features_folder, fasttext_token, caption_file):\n    self.data = []\n\n    # Setup the total feature file\n    data_paths = glob.glob(ocr_features_folder + '/*')\n    fasttext_token_ = np.load(fasttext_token, allow_pickle=True).tolist()\n    captions = np.load(caption_file, allow_pickle=True).tolist()\n\n    for path in tqdm(data_paths, desc='Load Data'):\n\n      # Load the feature from each file\n      image_name = path.split('/')[-1].split('.')[0]\n      ocr_feature = np.load(ocr_features_folder + '/' + image_name + '.npy', allow_pickle=True).tolist()\n      obj_info = np.load(obj_features_folder + '/' + image_name + '_info.npy', allow_pickle=True).tolist()\n      obj_feature = np.load(obj_features_folder + '/' + image_name + '.npy')\n\n      try:\n        sample = {\n            'id': image_name,\n            'captions': captions[image_name],\n\n            'ocr': {\n                'boxes': ocr_feature['boxes'],\n                'scores': ocr_feature['scores'], # The confidence score of the model's prediction\n                'size': (ocr_feature['weight'], ocr_feature['height']), # xmin, ymin, xmax, ymax\n                'texts': ocr_feature['texts'],\n                'fasttext_token': fasttext_token_[image_name],\n                'rec_features': ocr_feature['rec_features'],\n                'det_features': ocr_feature['det_features']\n                },\n\n            'obj': {\n                'boxes': obj_info['bbox'],\n                'scores': obj_info['cls_prob'], # The confidence score of the model's prediction\n                'size': (obj_info['image_width'], obj_info['image_height']), # xmin, ymin, xmax, ymax\n                'objects': obj_info['objects'], # With int type, the object's label\n                'features': obj_feature,\n            }\n        }\n      except:\n        continue\n\n      if self.convert_obj_bbx:\n        for i in range(len(sample['obj']['boxes'])):\n          sample['obj']['boxes'][i] = self.convert(sample['ocr']['size'], sample['obj']['boxes'][i]) # Use ocr-size because the obj-size is already scaled\n\n      self.data.append(sample)\n\n  def convert(self, size, box):\n    # size: width, height\n    # box: xmin, ymin, xmax, ymax\n    w, h = size\n    return (box[0] / w, box[1] / h, box[2] / w, box[3] / h)\n\n  def __getitem__(self, idx):\n    sample = self.data[idx]\n    return {\n            'id': sample['id'],\n            'captions': sample['captions'],\n            'obj_boxes': torch.tensor(sample['obj']['boxes']),\n            'obj_features': torch.tensor(sample['obj']['features']),\n            'ocr_texts': sample['ocr']['texts'],\n            'ocr_boxes': torch.tensor(sample['ocr']['boxes']),\n            'ocr_token_embeddings': torch.tensor(sample['ocr']['fasttext_token']) if len(sample['ocr']['fasttext_token']) > 0 else self.dummy_tensor,\n            'ocr_rec_features': torch.tensor(sample['ocr']['rec_features']),\n            'ocr_det_features': torch.tensor(sample['ocr']['det_features'])\n        }\n\n  def split_data(self, validation_size, test_size, random_state=42):\n\n    test_val_size = test_size + validation_size\n\n    # Split train and evaluation set\n    train_data, test_val_data = train_test_split(self.data,\n                                                 test_size=test_val_size,\n                                                 random_state=random_state)\n\n    # Split val and test\n    val_data, test_data = train_test_split(test_val_data,\n                                           test_size=test_size / test_val_size,\n                                           random_state=random_state)\n\n    return (ViTextCapsDataset(tokenizer=self.tokenizer, mask_value=self.mask_value, data=train_data),\n            ViTextCapsDataset(tokenizer=self.tokenizer, mask_value=self.mask_value, data=val_data),\n            ViTextCapsDataset(tokenizer=self.tokenizer, mask_value=self.mask_value, data=test_data))\n\n  def __len__(self):\n    return len(self.data)\n\n  def collate_fn(self, batch):\n\n    batch = dict(zip(batch[0].keys(), zip(*[d.values() for d in batch])))\n\n    # Convert obj list to tensor\n    obj_boxes_tensor = torch.stack(batch['obj_boxes'])\n    obj_features_tensor = torch.stack(batch['obj_features'])\n\n    # Convert ocr list to tensor\n    ocr_boxes_tensor = pad_sequence(batch['ocr_boxes'], batch_first=True, padding_value=self.mask_value)\n    ocr_token_embeddings_tensor = pad_sequence(batch['ocr_token_embeddings'], batch_first=True, padding_value=1)\n    ocr_rec_features_tensor = pad_sequence(batch['ocr_rec_features'], batch_first=True, padding_value=1)\n    ocr_det_features_tensor = pad_sequence(batch['ocr_det_features'], batch_first=True, padding_value=1)\n\n    captions_ = [choice(s) for s in batch['captions']]\n    raw_captions = batch['captions']\n    batch_id = batch['id']\n    texts_ = batch['ocr_texts']\n\n    vs = self.tokenizer.vocab_size + 1\n    labels_= []\n\n    # Captions to token\n    for i, caption in enumerate(captions_):\n      label_ = []\n\n      for token in word_tokenize(caption):\n\n          if token in texts_[i] and token not in self.tokenizer.get_vocab():\n            label_.append(texts_[i].index(token) + vs)\n          else:\n            label_ += self.tokenizer(token)['input_ids'][1: -1]\n\n      label_.append(2) # 2 is <eos> in tokenizer\n      labels_.append(torch.tensor(label_))\n\n    # Convert labels_ 2 tensor\n    labels_ = pad_sequence(labels_, batch_first=True, padding_value=1)\n\n    dec_mask = torch.ones_like(labels_)\n    dec_mask = dec_mask.masked_fill(labels_ == 1, 0) # batch_size, seq_length\n\n    # Get the ocr_attention_mask\n    ocr_attn_mask = torch.ones_like(ocr_boxes_tensor)\n    ocr_attn_mask = ocr_attn_mask.masked_fill(ocr_boxes_tensor == self.mask_value, 0)[:, :, 0] # batch_size, seq_length\n    ocr_boxes_tensor = ocr_boxes_tensor.masked_fill(ocr_boxes_tensor == self.mask_value, 1)\n\n    # Join attention_mask\n    obj_attn_mask = torch.ones(size=(obj_boxes_tensor.size(0), obj_boxes_tensor.size(1))) # batch_size, seq_length\n    join_attn_mask = torch.cat([obj_attn_mask, ocr_attn_mask, dec_mask], dim=-1)\n\n    return {\n          'id': batch_id,\n          'obj_boxes': obj_boxes_tensor,\n          'obj_features': obj_features_tensor,\n          'ocr_boxes': ocr_boxes_tensor,\n          'ocr_token_embeddings': ocr_token_embeddings_tensor,\n          'ocr_rec_features': ocr_rec_features_tensor,\n          'ocr_det_features': ocr_det_features_tensor,\n          'join_attn_mask': join_attn_mask,\n          'labels': labels_,\n          'texts': texts_,\n          'raw_captions': raw_captions\n    }","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:20:12.298684Z","iopub.execute_input":"2024-01-05T14:20:12.298969Z","iopub.status.idle":"2024-01-05T14:20:15.699484Z","shell.execute_reply.started":"2024-01-05T14:20:12.298943Z","shell.execute_reply":"2024-01-05T14:20:15.698519Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\nphobert_model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n\ndata = ViTextCapsDataset(tokenizer,\n                         '/kaggle/input/vitextcaps2/combine_obj_features',\n                         '/kaggle/input/vitextcaps2/combine_ocr_features',\n                         '/kaggle/input/vitextcaps2/token_fasttext_3.npy',\n                         '/kaggle/input/vitextcaps2/clean_captions.npy')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:20:15.700731Z","iopub.execute_input":"2024-01-05T14:20:15.701149Z","iopub.status.idle":"2024-01-05T14:22:35.388413Z","shell.execute_reply.started":"2024-01-05T14:20:15.701122Z","shell.execute_reply":"2024-01-05T14:22:35.387435Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74162843c2074798a70f0823c7a61d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd976649876746349f3a7dd488f71e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49eb7532155d43fca3947a8954d6fd89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f6af06b5cf4b6aa57693b396abf65f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6491d7b7fae6431b81016b98320f13e5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Load Data:   0%|          | 0/5289 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa446607a854c9e823aa2806e5db6a7"}},"metadata":{}}]},{"cell_type":"code","source":"train_data, val_data, test_data = data.split_data(validation_size=0.2, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.390230Z","iopub.execute_input":"2024-01-05T14:22:35.390828Z","iopub.status.idle":"2024-01-05T14:22:35.400373Z","shell.execute_reply.started":"2024-01-05T14:22:35.390791Z","shell.execute_reply":"2024-01-05T14:22:35.399658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_data), len(val_data), len(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.401524Z","iopub.execute_input":"2024-01-05T14:22:35.401789Z","iopub.status.idle":"2024-01-05T14:22:35.416724Z","shell.execute_reply.started":"2024-01-05T14:22:35.401766Z","shell.execute_reply":"2024-01-05T14:22:35.415881Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(3271, 935, 468)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=data.collate_fn)\nval_dataloader = DataLoader(val_data, batch_size=16, shuffle=False, collate_fn=data.collate_fn)\ntest_dataloader = DataLoader(test_data, batch_size=16, shuffle=False, collate_fn=data.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.418146Z","iopub.execute_input":"2024-01-05T14:22:35.418496Z","iopub.status.idle":"2024-01-05T14:22:35.428000Z","shell.execute_reply.started":"2024-01-05T14:22:35.418465Z","shell.execute_reply":"2024-01-05T14:22:35.427097Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ObjectEncoder(nn.Module):\n  def __init__(self, obj_in_dim, hidden_size, dropout_prob=0.1):\n    super().__init__()\n\n    # 2048 (FasterRCNN)\n    self.linear_obj_feat_to_mmt_in = nn.Linear(obj_in_dim, hidden_size)\n\n    # OBJ location feature\n    self.linear_obj_bbox_to_mmt_in = nn.Linear(4, hidden_size)\n\n    self.obj_feat_layer_norm = nn.LayerNorm(hidden_size)\n    self.obj_bbox_layer_norm = nn.LayerNorm(hidden_size)\n    self.dropout = nn.Dropout(dropout_prob)\n\n  def forward(self, obj_boxes, obj_features):\n\n    # Features to hidden size\n    obj_features = F.normalize(obj_features, dim=-1)\n\n    # Get obj features\n    obj_features = self.obj_feat_layer_norm(self.linear_obj_feat_to_mmt_in(obj_features))\n    obj_bbox_features = self.obj_bbox_layer_norm(self.linear_obj_bbox_to_mmt_in(obj_boxes))\n\n    return self.dropout(obj_features + obj_bbox_features) # batch_size, seq_length, hidden_size","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.429168Z","iopub.execute_input":"2024-01-05T14:22:35.429494Z","iopub.status.idle":"2024-01-05T14:22:35.439134Z","shell.execute_reply.started":"2024-01-05T14:22:35.429468Z","shell.execute_reply":"2024-01-05T14:22:35.438443Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class OCREncoder(nn.Module):\n  def __init__(self, ocr_in_dim, hidden_size, dropout_prob=0.1):\n    super().__init__()\n\n    # 300 (FastText) + 256 (rec_features) + 256 (det_features) = 812 # 768\n    self.linear_ocr_feat_to_mmt_in = nn.Linear(ocr_in_dim, hidden_size)\n\n    # OCR location feature\n    self.linear_ocr_bbox_to_mmt_in = nn.Linear(4, hidden_size)\n\n    self.ocr_feat_layer_norm = nn.LayerNorm(hidden_size)\n    self.ocr_bbox_layer_norm = nn.LayerNorm(hidden_size)\n    self.dropout = nn.Dropout(dropout_prob)\n\n  def forward(self, ocr_boxes, ocr_token_embeddings, ocr_rec_features, ocr_det_features):\n\n    # Normalize input\n    ocr_token_embeddings = F.normalize(ocr_token_embeddings, dim=-1)\n    ocr_rec_features = F.normalize(ocr_rec_features, dim=-1)\n    ocr_det_features = F.normalize(ocr_det_features, dim=-1)\n\n    # get OCR combine features\n    ocr_combine_features = torch.cat([ocr_token_embeddings, ocr_rec_features, ocr_det_features], dim=-1)\n    ocr_combine_features = self.ocr_feat_layer_norm(self.linear_ocr_feat_to_mmt_in(ocr_combine_features))\n\n    # Get OCR bbox features\n    ocr_bbox_features = self.ocr_bbox_layer_norm(self.linear_ocr_bbox_to_mmt_in(ocr_boxes))\n\n    return self.dropout(ocr_combine_features + ocr_bbox_features) # batch_size, seq_length, hidden_size","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.442568Z","iopub.execute_input":"2024-01-05T14:22:35.442890Z","iopub.status.idle":"2024-01-05T14:22:35.452693Z","shell.execute_reply.started":"2024-01-05T14:22:35.442839Z","shell.execute_reply":"2024-01-05T14:22:35.451894Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n  def __init__(self, d_model, max_len=512, dropout_prob=0.1):\n    super().__init__()\n\n    self.dropout = nn.Dropout(dropout_prob)\n\n    position_ids = torch.arange(max_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n    pe = torch.zeros(size=(1, max_len, d_model))\n    pe[0, :, 0::2] = torch.sin(position_ids / div_term)\n    pe[0, :, 1::2] = torch.cos(position_ids / div_term)\n    self.register_buffer('pe', pe)\n\n  def forward(self, x):\n    # x shape (batch_size, seq_length, d_model)\n    return x + self.pe[:, :x.size(1), :]\n\ndef _batch_gather(x, inds):\n    assert x.dim() == 3\n    batch_size = x.size(0)\n    length = x.size(1)\n    dim = x.size(2)\n\n    batch_offsets = torch.arange(batch_size, device=inds.device) * length\n    batch_offsets = batch_offsets.unsqueeze(-1)\n    assert batch_offsets.dim() == inds.dim()\n    results = F.embedding(batch_offsets + inds, x.view(batch_size * length, dim)) # batch_size, T, hidden_size\n    return results\n\nclass PrevPredEmbeddings(nn.Module):\n    def __init__(self, hidden_size, ln_eps=1e-12, dropout_prob=0.1):\n        super().__init__()\n\n        self.position_embeddings = PositionalEncoding(hidden_size)\n        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n        self.token_type_embeddings.weight.data = nn.Parameter(torch.cat([torch.zeros(1, hidden_size), torch.ones(1, hidden_size)]))\n\n        self.ans_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)\n        self.ocr_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)\n        self.emb_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)\n        self.emb_dropout = nn.Dropout(dropout_prob)\n\n    def forward(self, ans_emb, ocr_emb, labels):\n\n        batch_size = labels.size(0)\n        seq_length = labels.size(1)\n        ans_num = ans_emb.size(0)\n\n        # apply layer normalization to both answer embedding and OCR embedding\n        # before concatenation, so that they have the same scale\n        ans_emb = self.ans_layer_norm(ans_emb)\n        ocr_emb = self.ocr_layer_norm(ocr_emb)\n        assert ans_emb.size(-1) == ocr_emb.size(-1)\n\n        # Token type ids: 0 -- vocab; 1 -- OCR\n        token_type_embeddings = self.token_type_embeddings(labels.ge(ans_num).long()) # N, T, hidden_size\n        embeddings = self.emb_dropout(self.emb_layer_norm(self.position_embeddings(token_type_embeddings)))\n\n        return _batch_gather(torch.cat([ans_emb.unsqueeze(0).expand(batch_size, -1, -1), ocr_emb], dim=1), labels) + embeddings","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.453968Z","iopub.execute_input":"2024-01-05T14:22:35.454272Z","iopub.status.idle":"2024-01-05T14:22:35.471800Z","shell.execute_reply.started":"2024-01-05T14:22:35.454246Z","shell.execute_reply":"2024-01-05T14:22:35.471075Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  def __init__(self, d_model, n_heads, d_k, causal=False):\n    super().__init__()\n\n    self.n_heads = n_heads\n    self.d_k = d_k\n\n    self.query = nn.Linear(d_model, n_heads * d_k)\n    self.key = nn.Linear(d_model, n_heads * d_k)\n    self.value = nn.Linear(d_model, n_heads * d_k)\n\n    self.fc = nn.Linear(n_heads * d_k, d_model)\n    self.causal = causal\n\n  def forward(self, x, dec_size, attention_mask=None):\n    # x shape (batch_size, seq_length, d_model)\n\n    N = x.size(0)\n    T = x.size(1)\n\n    # Pass through linear to get q, k, v\n    q = self.query(x).view(N, T, self.n_heads, self.d_k).transpose(1, 2) # batch_size, n_heads, T, d_k\n    k = self.key(x).view(N, T, self.n_heads, self.d_k).transpose(1, 2) # batch_size, n_heads, T, d_k\n    v = self.query(x).view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n\n    # Get the attention scores\n    attn_scores = (q @ k.mT) / math.sqrt(self.d_k) # batch_size, n_heads, T_dec, T_enc\n\n    # Mask the padding values and (if causal)\n    if attention_mask is not None:\n      attn_scores = attn_scores.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf'))\n\n    if self.causal:\n      causal_mask = torch.tril(torch.ones(dec_size, dec_size))\n      extend_causal_mask = torch.ones((T, T))\n      extend_causal_mask[:, -dec_size:] = torch.cat([torch.zeros((T - dec_size, dec_size)), causal_mask])\n      extend_causal_mask = extend_causal_mask.to(attn_scores.device)\n\n      attn_scores = attn_scores.masked_fill(extend_causal_mask[None, None, :, :]==0, float('-inf'))\n\n    # Get the attention weights\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n\n    # Get the values\n    A = attn_weights @ v # batch_size, n_heads, T_dec, d_k\n\n    # Reshape to batch_size, T_dec, n_heads * d_k\n    A = A.transpose(1, 2).contiguous().view(N, T, self.n_heads * self.d_k)\n\n    return self.fc(A)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.473034Z","iopub.execute_input":"2024-01-05T14:22:35.473533Z","iopub.status.idle":"2024-01-05T14:22:35.490308Z","shell.execute_reply.started":"2024-01-05T14:22:35.473499Z","shell.execute_reply":"2024-01-05T14:22:35.489565Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n  def __init__(self, d_model, n_heads, d_k, dropout_prob=0.1):\n    super().__init__()\n\n    self.mha = MultiHeadAttention(d_model, n_heads, d_k, causal=True)\n    self.ln1 = nn.LayerNorm(d_model)\n    self.ln2 = nn.LayerNorm(d_model)\n    self.ffn = nn.Sequential(\n        nn.Linear(d_model, d_model * 4),\n        nn.GELU(),\n        nn.Linear(d_model * 4, d_model),\n        nn.Dropout(dropout_prob),\n    )\n    self.dropout = nn.Dropout(dropout_prob)\n\n  def forward(self, x, dec_size, attention_mask=None):\n\n    x = self.ln1(x + self.mha(x, dec_size, attention_mask))\n    x = self.ln2(x + self.ffn(x))\n    return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.491449Z","iopub.execute_input":"2024-01-05T14:22:35.491700Z","iopub.status.idle":"2024-01-05T14:22:35.505155Z","shell.execute_reply.started":"2024-01-05T14:22:35.491678Z","shell.execute_reply":"2024-01-05T14:22:35.504241Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n  def __init__(self, d_model, n_heads, d_k, n_layers):\n    super().__init__()\n\n    self.transformer_blocks = nn.Sequential(*[DecoderBlock(d_model, n_heads, d_k) for _ in range(n_layers)])\n\n  def forward(self, x, dec_size, attention_mask=None):\n\n    for block in self.transformer_blocks:\n      x = block(x, dec_size, attention_mask)\n\n    return x # N, T, hidden_size","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.506382Z","iopub.execute_input":"2024-01-05T14:22:35.506980Z","iopub.status.idle":"2024-01-05T14:22:35.520894Z","shell.execute_reply.started":"2024-01-05T14:22:35.506945Z","shell.execute_reply":"2024-01-05T14:22:35.520065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class MMT(nn.Module):\n  def __init__(self, d_model, n_heads, d_k, n_layers):\n    super().__init__()\n    self.prev_pred_embeddings = PrevPredEmbeddings(d_model)\n    self.encoder = Decoder(d_model, n_heads, d_k, n_layers)\n\n  def forward(self, obj_emb, fixed_ans_emb, ocr_emb, prev_inds, attention_mask):\n\n    dec_emb = self.prev_pred_embeddings(fixed_ans_emb, ocr_emb, prev_inds)\n\n    encoder_inputs = torch.cat([obj_emb, ocr_emb, dec_emb], dim=1) # batch_size, T (obj + ocr + dec), 768\n\n    # Get the size for each cause we gonna need that in the pointer network\n    obj_max_num = obj_emb.size(1)\n    ocr_max_num = ocr_emb.size(1)\n    dec_max_num = dec_emb.size(1)\n\n    # offsets of each modality in the joint embedding space\n    ocr_begin = obj_max_num\n    ocr_end = ocr_begin + ocr_max_num\n\n    encoder_outputs = self.encoder(encoder_inputs, dec_max_num, attention_mask) # N, T, hidden_size\n\n    # mmt_dec_output = encoder_outputs[:, ocr_end:, :] # batch_size, dec_max_num, hidden_size\n    # mmt_ocr_output = encoder_outputs[:, ocr_begin: ocr_end, :] # batch_size, ocr_max_num, hidden_size\n\n    return encoder_outputs[:, ocr_begin: ocr_end, :], encoder_outputs[:, ocr_end:, :]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.522080Z","iopub.execute_input":"2024-01-05T14:22:35.522420Z","iopub.status.idle":"2024-01-05T14:22:35.533434Z","shell.execute_reply.started":"2024-01-05T14:22:35.522369Z","shell.execute_reply":"2024-01-05T14:22:35.532534Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class OcrPtrNet(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, query_inputs, key_inputs, attention_mask):\n\n        # query_layer = self.query(query_inputs) # batch_size, dec_max_num, hidden_size\n        # key_layer = self.key(key_inputs) # batch_size, ocr_max_num, hidden_size\n\n        scores = self.query(query_inputs) @ self.key(key_inputs).mT\n        scores = scores / math.sqrt(self.hidden_size) # batch_size, dec_max_num, ocr_max_num\n\n        scores = scores.masked_fill(attention_mask[:, None, :] == 0, -1e4)\n        return scores","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.534590Z","iopub.execute_input":"2024-01-05T14:22:35.534847Z","iopub.status.idle":"2024-01-05T14:22:35.553192Z","shell.execute_reply.started":"2024-01-05T14:22:35.534826Z","shell.execute_reply":"2024-01-05T14:22:35.552428Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class M4C(nn.Module):\n  def __init__(self,\n               obj_in_dim,\n               ocr_in_dim,\n               hidden_size,\n               n_heads,\n               d_k,\n               n_layers,\n               vocab_size,\n               fixed_ans_emb):\n    super().__init__()\n    self.obj_encoder = ObjectEncoder(obj_in_dim=obj_in_dim, hidden_size=hidden_size)\n    self.ocr_encoder = OCREncoder(ocr_in_dim=ocr_in_dim, hidden_size=hidden_size)\n    self.mmt = MMT(d_model=hidden_size, n_heads=n_heads, d_k=d_k, n_layers=n_layers)\n    self.ocr_ptn = OcrPtrNet(hidden_size=hidden_size)\n    self.classifier = nn.Linear(hidden_size, vocab_size)\n    self.fixed_ans_emb = fixed_ans_emb\n    self.finetune_modules = [{\"module\": self.obj_encoder.linear_obj_feat_to_mmt_in, \"lr_scale\": 0.1},\n                             {\"module\": self.ocr_encoder.linear_ocr_feat_to_mmt_in, \"lr_scale\": 0.1},\n                             {\"module\": self.mmt, \"lr_scale\": 1}]\n\n  def get_optimizer_parameters(self, base_lr):\n        optimizer_param_groups = []\n\n        # collect all the parameters that need different/scaled lr\n        finetune_params_set = set()\n        for m in self.finetune_modules:\n            optimizer_param_groups.append(\n                {\n                    \"params\": list(m[\"module\"].parameters()),\n                    \"lr\": base_lr * m[\"lr_scale\"],\n                }\n            )\n            finetune_params_set.update(list(m[\"module\"].parameters()))\n        # remaining_params are those parameters w/ default lr\n        remaining_params = [\n            p for p in self.parameters() if p not in finetune_params_set\n        ]\n        # put the default lr parameters at the beginning\n        # so that the printed lr (of group 0) matches the default lr\n        optimizer_param_groups.insert(0, {\"params\": remaining_params})\n\n        return optimizer_param_groups\n\n  def forward(self, sample, device='cpu'):\n\n    obj_emb = self.obj_encoder(sample['obj_boxes'].to(device), sample['obj_features'].to(device))\n    ocr_emb = self.ocr_encoder(sample['ocr_boxes'].to(device),\n                               sample['ocr_token_embeddings'].to(device),\n                               sample['ocr_rec_features'].to(device),\n                               sample['ocr_det_features'].to(device))\n\n    # Create decoder inputs\n    dec_input = sample['labels'].clone().detach().roll(shifts=1, dims=1)\n    dec_input[:, 0] = 0 # <s> token\n\n    mmt_ocr_output, mmt_dec_output = self.mmt(obj_emb,\n                                              self.fixed_ans_emb,\n                                              ocr_emb,\n                                              dec_input.to(device),\n                                              sample['join_attn_mask'].to(device))\n\n    ocr_begin = obj_emb.size(1)\n    ocr_end = obj_emb.size(1) + mmt_ocr_output.size(1)\n\n    # dynamic_ocr_scores = self.ocr_ptn(mmt_dec_output, mmt_ocr_output, sample['join_attn_mask'][:, ocr_begin: ocr_end].to(device))\n    # fixed_scores = self.classifier(mmt_dec_output)\n\n    scores = torch.cat([self.classifier(mmt_dec_output),\n                        self.ocr_ptn(mmt_dec_output, mmt_ocr_output, sample['join_attn_mask'][:, ocr_begin: ocr_end].to(device))],\n                        dim=-1)\n\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.554384Z","iopub.execute_input":"2024-01-05T14:22:35.554662Z","iopub.status.idle":"2024-01-05T14:22:35.569791Z","shell.execute_reply.started":"2024-01-05T14:22:35.554638Z","shell.execute_reply":"2024-01-05T14:22:35.568968Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def convert_prediction_to_ans(prediction_list, sample_texts, tokenizer):\n    raw_predicts = []\n\n    for l, text in zip(prediction_list, sample_texts):\n        raw_predict = []\n        vocab_predict = []\n\n        for w in l:\n            if w > 64000: # Kiểm tra nếu w là ocr token\n                if len(vocab_predict) > 0: # Kiểm tra nếu trước đó đã có các vocab token\n                    raw_predict.append(tokenizer.decode(vocab_predict))\n                    vocab_predict = []\n\n                raw_predict.append(text[w - 64001])\n            else:\n                vocab_predict += [w]\n\n        if len(vocab_predict) > 0:\n                raw_predict.append(tokenizer.decode(vocab_predict))\n\n        caption = ' '.join(raw_predict)\n        raw_predicts.append(caption)\n\n    return raw_predicts","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.571063Z","iopub.execute_input":"2024-01-05T14:22:35.571372Z","iopub.status.idle":"2024-01-05T14:22:35.587087Z","shell.execute_reply.started":"2024-01-05T14:22:35.571341Z","shell.execute_reply":"2024-01-05T14:22:35.586432Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n\n    model.eval()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.588138Z","iopub.execute_input":"2024-01-05T14:22:35.588382Z","iopub.status.idle":"2024-01-05T14:22:35.599125Z","shell.execute_reply.started":"2024-01-05T14:22:35.588360Z","shell.execute_reply":"2024-01-05T14:22:35.598425Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = load_checkpoint('/kaggle/input/vitextcapscheckpointfinalform/checkpoint (1).pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:35.600290Z","iopub.execute_input":"2024-01-05T14:22:35.600605Z","iopub.status.idle":"2024-01-05T14:22:52.022643Z","shell.execute_reply.started":"2024-01-05T14:22:35.600571Z","shell.execute_reply":"2024-01-05T14:22:52.021826Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"sample = next(iter(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:52.024097Z","iopub.execute_input":"2024-01-05T14:22:52.024824Z","iopub.status.idle":"2024-01-05T14:22:52.271065Z","shell.execute_reply.started":"2024-01-05T14:22:52.024786Z","shell.execute_reply":"2024-01-05T14:22:52.270168Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def decode_prediction(obj_boxes,\n                      obj_features,\n                      ocr_boxes,\n                      ocr_tokens,\n                      ocr_token_embeddings,\n                      ocr_rec_features,\n                      ocr_det_features, \n                      join_obj_ocr_mask, \n                      tokenizer, \n                      fixed_ans_emb):\n    \n    pred_ans = [tokenizer.bos_token_id]\n    dec_input = torch.tensor([pred_ans])\n\n    obj_emb = model.obj_encoder(obj_boxes=obj_boxes.unsqueeze(0),\n                                obj_features=obj_features.unsqueeze(0))\n\n    ocr_emb = model.ocr_encoder(ocr_boxes=ocr_boxes.unsqueeze(0),\n                                ocr_token_embeddings=ocr_token_embeddings.unsqueeze(0),\n                                ocr_rec_features=ocr_rec_features.unsqueeze(0),\n                                ocr_det_features=ocr_det_features.unsqueeze(0))\n\n    join_obj_ocr_mask = join_obj_ocr_mask.unsqueeze(0)\n    ocr_begin = obj_emb.size(1)\n    ocr_end = ocr_begin + ocr_emb.size(1)\n    \n    for i in tqdm(range(50)):\n    \n        dec_mask = torch.ones((1, dec_input.size(1)), dtype=torch.int64)\n        attn_mask = torch.cat([join_obj_ocr_mask, dec_mask], dim=-1)\n\n        mmt_ocr_output, mmt_dec_output = model.mmt(obj_emb,\n                                                  fixed_ans_emb,\n                                                  ocr_emb,\n                                                  dec_input,\n                                                  attn_mask)\n\n#         dynamic_ocr_scores = model.ocr_ptn(mmt_dec_output, mmt_ocr_output, join_obj_ocr_mask[0, ocr_begin: ocr_end].unsqueeze(0))\n        fixed_scores = model.classifier(mmt_dec_output)\n#         scores = torch.cat([fixed_scores, dynamic_ocr_scores], dim=-1)\n        scores = fixed_scores\n        scores = scores.argmax(dim=-1)\n        scores = scores[0][-1].item()\n\n        if scores == tokenizer.eos_token_id:\n            break\n\n        pred_ans.append(scores)\n        dec_input = torch.tensor([pred_ans])\n        \n    decode_ans = convert_prediction_to_ans([pred_ans], [ocr_tokens], tokenizer)\n    return decode_ans","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:52.272116Z","iopub.execute_input":"2024-01-05T14:22:52.272377Z","iopub.status.idle":"2024-01-05T14:22:52.282653Z","shell.execute_reply.started":"2024-01-05T14:22:52.272354Z","shell.execute_reply":"2024-01-05T14:22:52.281822Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sample = next(iter(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:52.283617Z","iopub.execute_input":"2024-01-05T14:22:52.283854Z","iopub.status.idle":"2024-01-05T14:22:52.471513Z","shell.execute_reply.started":"2024-01-05T14:22:52.283833Z","shell.execute_reply":"2024-01-05T14:22:52.470711Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"i = 10 \nobj_length = sample['obj_boxes'][i].size(0)\nocr_length = sample['ocr_boxes'][i].size(0)\n\ndecode_prediction(obj_boxes=sample['obj_boxes'][i],\n                  obj_features=sample['obj_features'][i],\n                  ocr_boxes=sample['ocr_boxes'][i],\n                  ocr_tokens=sample['texts'][i],\n                  ocr_token_embeddings=sample['ocr_token_embeddings'][i],\n                  ocr_rec_features=sample['ocr_rec_features'][i],\n                  ocr_det_features=sample['ocr_det_features'][i],\n                  join_obj_ocr_mask=sample['join_attn_mask'][i, :obj_length + ocr_length], \n                  tokenizer=tokenizer, \n                  fixed_ans_emb=phobert_model.embeddings.word_embeddings.weight.data)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T14:22:52.472660Z","iopub.execute_input":"2024-01-05T14:22:52.472949Z","iopub.status.idle":"2024-01-05T14:23:09.552268Z","shell.execute_reply.started":"2024-01-05T14:22:52.472923Z","shell.execute_reply":"2024-01-05T14:23:09.551317Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061facec3f33432f99c208f767870f9f"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['<s> mọi người đang mua sắm.']"},"metadata":{}}]},{"cell_type":"code","source":"obj_length = sample['obj_boxes'][0].size(0)\nocr_length = sample['ocr_boxes'][0].size(0)\nsample['join_attn_mask'][0, : obj_length + ocr_length].shape","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:28:18.111947Z","iopub.execute_input":"2024-01-03T06:28:18.112789Z","iopub.status.idle":"2024-01-03T06:28:18.119376Z","shell.execute_reply.started":"2024-01-03T06:28:18.112758Z","shell.execute_reply":"2024-01-03T06:28:18.118504Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"torch.Size([121])"},"metadata":{}}]},{"cell_type":"code","source":"ocr_length","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:27:53.125717Z","iopub.execute_input":"2024-01-03T06:27:53.126593Z","iopub.status.idle":"2024-01-03T06:27:53.132083Z","shell.execute_reply.started":"2024-01-03T06:27:53.126559Z","shell.execute_reply":"2024-01-03T06:27:53.131266Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"85"},"metadata":{}}]},{"cell_type":"code","source":"obj_length","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:28:03.583216Z","iopub.execute_input":"2024-01-03T06:28:03.584183Z","iopub.status.idle":"2024-01-03T06:28:03.590534Z","shell.execute_reply.started":"2024-01-03T06:28:03.584140Z","shell.execute_reply":"2024-01-03T06:28:03.589461Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"36"},"metadata":{}}]},{"cell_type":"raw","source":"# Assuming you have the following sequence lengths for each modality\ntxt_max_num = 10\nobj_max_num = 15\nocr_max_num = 12\ndec_max_num = 5\n\n# Assuming a batch size of 2 for demonstration purposes\nbatch_size = 2\n\n# Creating random binary masks for each modality\ntxt_mask = torch.randint(2, size=(batch_size, txt_max_num))\nobj_mask = torch.randint(2, size=(batch_size, obj_max_num))\nocr_mask = torch.randint(2, size=(batch_size, ocr_max_num))\n\n# Creating a zero mask for decoding steps\ndec_mask = torch.zeros(batch_size, dec_max_num)","metadata":{}},{"cell_type":"code","source":"# Assuming you have the following sequence lengths for each modality\ntxt_max_num = 10\nobj_max_num = 15\nocr_max_num = 12\ndec_max_num = 5\n\n# Assuming a batch size of 2 for demonstration purposes\nbatch_size = 2\n\n# Creating random binary masks for each modality\ntxt_mask = torch.randint(2, size=(batch_size, txt_max_num))\nobj_mask = torch.randint(2, size=(batch_size, obj_max_num))\nocr_mask = torch.randint(2, size=(batch_size, ocr_max_num))\n\n# Creating a zero mask for decoding steps\ndec_mask = torch.zeros(batch_size, dec_max_num)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:18:36.023068Z","iopub.execute_input":"2024-01-03T07:18:36.023429Z","iopub.status.idle":"2024-01-03T07:18:36.030982Z","shell.execute_reply.started":"2024-01-03T07:18:36.023402Z","shell.execute_reply":"2024-01-03T07:18:36.030049Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"def _get_causal_mask(seq_length, device):\n    # generate a lower triangular mask\n    mask = torch.zeros(seq_length, seq_length, device=device)\n    for i in range(seq_length):\n        for j in range(i + 1):\n            mask[i, j] = 1.0\n    return mask\n\ndef generate_extended_attention_mask(attention_mask, dec_max_num, device):\n    to_seq_length = attention_mask.size(1)\n    from_seq_length = to_seq_length\n\n    # generate the attention mask similar to prefix LM\n    # all elements can attend to the elements in encoding steps\n    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n    extended_attention_mask = extended_attention_mask.repeat(1, 1, from_seq_length, 1)\n\n    # decoding step elements can attend to themselves in a causal manner\n    extended_attention_mask[:, :, -dec_max_num:, -dec_max_num:] = _get_causal_mask(\n        dec_max_num, device\n    )\n\n    # flip the mask, so that invalid attention pairs have -10000.\n    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n    return extended_attention_mask\n\n# Example usage:\n# Assuming you have the necessary inputs: txt_mask, obj_mask, ocr_mask, dec_mask\n# (you can replace these with the actual masks from your data)\n# and dec_max_num (the number of decoding steps)\nattention_mask = torch.cat([txt_mask, obj_mask, ocr_mask, dec_mask], dim=1)\nextended_attention_mask = generate_extended_attention_mask(\n    attention_mask, dec_max_num, 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:19:27.249391Z","iopub.execute_input":"2024-01-03T07:19:27.249764Z","iopub.status.idle":"2024-01-03T07:19:27.259333Z","shell.execute_reply.started":"2024-01-03T07:19:27.249734Z","shell.execute_reply":"2024-01-03T07:19:27.258325Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"extended_attention_mask[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:19:59.978660Z","iopub.execute_input":"2024-01-03T07:19:59.979283Z","iopub.status.idle":"2024-01-03T07:19:59.987781Z","shell.execute_reply.started":"2024-01-03T07:19:59.979253Z","shell.execute_reply":"2024-01-03T07:19:59.986771Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"tensor([[[    -0., -10000., -10000.,  ..., -10000., -10000., -10000.],\n         [    -0., -10000., -10000.,  ..., -10000., -10000., -10000.],\n         [    -0., -10000., -10000.,  ..., -10000., -10000., -10000.],\n         ...,\n         [    -0., -10000., -10000.,  ...,     -0., -10000., -10000.],\n         [    -0., -10000., -10000.,  ...,     -0.,     -0., -10000.],\n         [    -0., -10000., -10000.,  ...,     -0.,     -0.,     -0.]]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}